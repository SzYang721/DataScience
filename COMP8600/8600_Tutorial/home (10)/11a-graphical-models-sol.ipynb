{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models (Bayesian Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumed knowledge\n",
    "- Bayesian Networks (lecture in Week 10)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Basic operations and definitions of Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial mainly includes three parts: Bayesian Networks (BN), Markov Random Field (MRF) and Sum Product Algorithm (Factor Graph). Before diving into the graphical models, we will first review some basic probability concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Probability\n",
    "\n",
    "### Discrete Probability\n",
    "\n",
    "Recall the meaning of the following terms:\n",
    "* Joint probability distribution\n",
    "* Marginal distribution\n",
    "* Conditional distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "See Section 1.2 of the Bishop's textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following table defining the joint probability distribution of two variables $A$ and $B$.\n",
    "\n",
    "|  | A=$\\square$ | A=$\\bigcirc$ | A = $\\clubsuit$ | A = $\\heartsuit$ | A = $\\triangle$ |\n",
    "|--|:--:|:--:|:--:|:--:|:--:|\n",
    "|**B**=$p$|0.01|0.01|0.12|0.01|0.14|\n",
    "|**B**=$q$|0.03|0.15|0.01|0.01|0.01|\n",
    "|**B**=$r$|0.13|0.11|0.07|0.18|0.01|\n",
    "\n",
    "Compute the following distributions:\n",
    "* $p(B=p | A = \\bigcirc)$\n",
    "* $p(B | A = \\bigcirc)$\n",
    "* $p(B)$\n",
    "\n",
    "You may do this calculation by hand or using python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(B=p|A=o) =  0.037037037037037035\n",
      "p(B|A=o) =  [0.03703704 0.55555556 0.40740741]\n",
      "p(B) =  [0.29 0.21 0.5 ]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "P_AB = np.array([[0.01, 0.01, 0.12, 0.01, 0.14],\n",
    "                 [0.03, 0.15, 0.01, 0.01, 0.01],\n",
    "                 [0.13, 0.11, 0.07, 0.18, 0.01]])\n",
    "\n",
    "print('p(B=p|A=o) = ' , P_AB[0,1]/np.sum(P_AB[:,1]))\n",
    "\n",
    "print('p(B|A=o) = ', P_AB[:,1]/np.sum(P_AB[:,1]))\n",
    "\n",
    "print('p(B) = ', np.sum(P_AB, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "\n",
    "Recall that there are only two rules of probability, the sum rule and the product rule. Using these two rules, prove Bayes rule.\n",
    "\n",
    "$$p(Y|X) = \\frac{p(X|Y)p(Y)}{\\sum_Y p(X,Y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Sum rule\n",
    "$$p(X) = \\sum_Y p(X,Y)$$\n",
    "Product rule\n",
    "$$p(X,Y) = p(Y|X) p(X)$$\n",
    "\n",
    "Observe that the order of variables in joint distributions is irrelevant.\n",
    "$$p(X,Y) = p(Y,X)$$\n",
    "Using the product rule on both sides,\n",
    "$$p(Y|X)p(X) = p(X|Y)p(Y)$$\n",
    "Dividing both sides by $p(X)$,\n",
    "$$p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}$$\n",
    "By the sum rule,\n",
    "$$p(Y|X) = \\frac{p(X|Y)p(Y)}{\\sum_Y p(X,Y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical verification of Bayes rule\n",
    "\n",
    "Using the distribution $p(A,B)$ above, verify the Bayes rule i.e. \n",
    "\n",
    "$$p(A|B) = \\frac{p(B|A)p(A)}{\\sum_A p(A,B)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHS: p(A|B)\n",
      "[[0.03448276 0.03448276 0.4137931  0.03448276 0.48275862]\n",
      " [0.14285714 0.71428571 0.04761905 0.04761905 0.04761905]\n",
      " [0.26       0.22       0.14       0.36       0.02      ]]\n",
      "RHS:\n",
      "[[0.03448276 0.03448276 0.4137931  0.03448276 0.48275862]\n",
      " [0.14285714 0.71428571 0.04761905 0.04761905 0.04761905]\n",
      " [0.26       0.22       0.14       0.36       0.02      ]]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def joint2cond(P, row_cond=True):\n",
    "    if row_cond:\n",
    "        totals = np.sum(P, axis=0)\n",
    "        return P/totals\n",
    "    else:\n",
    "        totals = np.sum(P, axis=1)\n",
    "        return (P.T/totals).T\n",
    "\n",
    "\n",
    "P_B = np.sum(P_AB, axis=1)\n",
    "P_BgA = joint2cond(P_AB, row_cond=True)\n",
    "P_A = np.sum(P_AB, axis=0)\n",
    "\n",
    "print('LHS: p(A|B)')\n",
    "LHS = joint2cond(P_AB, row_cond=False)\n",
    "print(LHS)\n",
    "\n",
    "numerator = P_BgA * P_A\n",
    "RHS = (numerator.T/P_B).T\n",
    "print('RHS:')\n",
    "print(RHS)\n",
    "\n",
    "assert (LHS == RHS).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent random variables\n",
    "\n",
    "Consider the following problem with 5 random variables.\n",
    "* **A**ches with states (False, True)\n",
    "* **B**ronchitis with states (none, mild, severe)\n",
    "* **C**ough with states (False, True)\n",
    "* **D**isease with states (healthy, carrier, sick, recovering)\n",
    "* **E**mergency with states (False, True)\n",
    "\n",
    "How much memory is needed to store the joint probability distribution if:\n",
    "* All variables are dependent?\n",
    "* All variables are independent?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* If all variables are dependent, we need to store the joint probability distribution $P(A,B,C,D,E): 2 \\times 3 \\times 2 \\times 4 \\times 2 = 96$. Actually, the last value of the joint distribution can be derived from all other known ones, so we only need $95$.\n",
    "\n",
    "* If all variables are independent, the joint distribution can be factorised as $P(A,B,C,D,E) = P(A)P(B)P(C)P(D)P(E): 2 + 3 + 2 + 4 + 2 = 13$ If we also exclude the last value for each distribution, we only need store $13-5=8$ values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Network\n",
    "\n",
    "Bayesian Network is directed graphical model expressing causal relationship between variables.\n",
    "\n",
    "Consider the following graphical model. Anwser the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bayesian_Network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Write down the joint factorisation  for the above graph. \n",
    "\n",
    "(2) How much memory is needed to store the joint probability distribution?\n",
    "\n",
    "\n",
    "### Conditional Independence (D-seperation)\n",
    "\n",
    "If all paths are blocked between nodes $X$, $Y$ when a set of nodes $Z$ is observed, then $X$ is $d$-separated from $Y$ by $Z$ and $X \\perp Y | Z$. A path is blocked if it includes a node such that:\n",
    "- the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the node is in $Z$\n",
    "- the arrows meet head-to-head at the node, and neither the node, nor any of its descendants, is in $Z$.\n",
    "\n",
    "(4) Identify and prove whether the conditional independences holds for the following cases: \n",
    "\n",
    "* A and D, when B is observed.\n",
    "\n",
    "* B and C, when none of the variables are observed.\n",
    "\n",
    "* B and C, when E is observed.\n",
    "\n",
    "* A and C, when none of the variables are observed.\n",
    "\n",
    "* A and C, when B is observed.\n",
    "\n",
    "* A and E, when D is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "(1) $P(B)P(C)P(A|B)P(D|B,C)P(E|D)$\n",
    "\n",
    "(2) Memory: P(B) has 3 values, P(C) has 2 values, P(A∣B) has 2×3=6 values, P(D∣B,C) has 4×3×2=24 values, P(E∣D) has 4×2=8 values for a total of 43 values. As previously discussed the minimum storage requirement is actually 38 since the last value of each of these distributions could be removed.\n",
    "\n",
    "(3)\n",
    "- $A\\perp D | B$\n",
    "\n",
    "- $B\\perp C | \\emptyset$\n",
    "\n",
    "- Not $B⊥C∣E$\n",
    "\n",
    "- $A\\perp C | \\emptyset$\n",
    "\n",
    "- $A\\perp C | B$\n",
    "- $A\\perp E | D$\n",
    "\n",
    "Example proof: \n",
    "$A⊥D∣B$:\n",
    "\\begin{align}\n",
    "p(A,D | B) &= \\frac{p(A,B,D)}{p(B)}\\\\\n",
    "&=\\frac{p(A|B)\\, p(D|B)\\, p(B)}{p(B)}\\\\\n",
    "&=p(A|B)\\,p(D|B)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distributions for BN\n",
    "\n",
    "Consider the following tables.\n",
    "\n",
    "|$p(B)$ | B=n | B=m | B=s |\n",
    "|:-----:|:--:|:--:|:--:|\n",
    "|marginal| 0.97 | 0.01 | 0.02 |\n",
    "\n",
    "<br />\n",
    "\n",
    "|$p(C)$ | C=False | C=True |\n",
    "|:-----:|:--:|:--:|\n",
    "|marginal| 0.7 | 0.3 |\n",
    "\n",
    "<br />\n",
    "\n",
    "| $p(A\\mid B)$ | B=n | B=m | B=s |\n",
    "|:-----:|:--:|:--:|:--:|\n",
    "|**A**=False |0.9|0.8|0.3|\n",
    "|**A**=True |0.1|0.2|0.7|\n",
    "\n",
    "<br />\n",
    "\n",
    "| $p(D \\mid B,C)$ | B=n, C=F | B=m, C=F | B=s, C=F | B=n, C=T | B=m, C=T | B=s, C=T |\n",
    "|:-----:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|**D**=healthy   |0.9 |0.8 |0.1 |  0.3 |0.4 |0.01|\n",
    "|**D**=carrier   |0.08|0.17|0.01|  0.05|0.05|0.01|\n",
    "|**D**=sick      |0.01|0.01|0.87|  0.05|0.15|0.97|\n",
    "|**D**=recovering|0.01|0.02|0.02|  0.6 |0.4 |0.01|\n",
    "\n",
    "<br />\n",
    "\n",
    "| $p(E \\mid D)$ | D=h | D=c | D=s | D=r |\n",
    "|:-----:|:--:|:--:|:--:| :--:|\n",
    "|**E**=False |0.99|0.99|0.4|0.9|\n",
    "|**E**=True |0.01|0.01|0.6|0.1|\n",
    "\n",
    "\n",
    "\n",
    "Compute the following:\n",
    "* $p(A,B,C,D,E)$\n",
    "\n",
    "* $p(E)$\n",
    "\n",
    "* $p(E|B=s)$\n",
    "\n",
    "* $p(E|B=s, C=T)$\n",
    "\n",
    "Note that there are two ways of arriving at the distributions:\n",
    "1. By computing $p(A,B,C,D,E)$ and marginalising and conditioning appropriately\n",
    "2. By only computing the required distributions directly using the graphical model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\n",
    "The sample solution illustrates the key points of the two ways of computing the distributions.\n",
    "\n",
    "(1) By computing p(A,B,C,D,E) and marginalising and conditioning appropriately\n",
    "\n",
    "* $P(A,B,C,D,E) = P(B)P(C)P(A|B)P(D|B,C)P(E|D)$\n",
    "\n",
    "* $P(E) = \\sum_{A,B,C,D} P(A,B,C,D,E)$\n",
    "\n",
    "* $P(E|B=s) = \\frac{P(E, B =s)}{P(B = s)} = \\frac{\\sum_{A, C, D}P(A, B =s, B, C, D, E)}{P(B = s)}$\n",
    "\n",
    "* $p(E|B=s, C=T) = \\frac{P(E, B =s, C = T)}{P(B = s, C = T)} = \\frac{\\sum_{A, D}P(A, B =s, C = T, D, E)}{P(B = s, C = T)} = \\frac{\\sum_{A, D}P(A, B =s, C = T, D, E)}{P(B = s) P(C = T)}$\n",
    "\n",
    "(2) By only computing the required distributions directly using the graphical model structure.\n",
    "\n",
    "* $P(A,B,C,D,E) = P(B)P(C)P(A|B)P(D|B,C)P(E|D)$\n",
    "\n",
    "* $P(E) = \\sum_{B,C,D} P(B)P(C)P(D|B,C)P(E|D)$\n",
    "\n",
    "* $P(E|B=s) = \\frac{P(E, B =s)}{ P(B =s)}  =  \\frac{\\sum_{C,D} P(B = s)P(C)P(D|B = s,C)P(E|D)}{P(B =s)} = \\sum_{C,D} P(C)P(D|B = s,C)P(E|D)$\n",
    "\n",
    "* $P(E|B=s, C=T) = \\frac{P(E, B =s, C = T)}{P(B = s, C = T)} = \\frac{\\sum_{D}P(B =s)P(C = T)P(D|B=s, C=T)P(E|D)}{P(B = s) P(C = T)} = \\sum_{D} P(D|B=s, C=T)P(E|D))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling a Discrete Probability Mass Function (PMF)\n",
    "1. Code a class which represents a PMF, and has a ``sample`` method which draws a sample from the PMF.\n",
    "2. Use it to draw samples from $p(C)$ and empirically compute and print the probability that your sampler returns the value ``False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "class PMF(object):\n",
    "    '''Class for sampling from PMF'''\n",
    "\n",
    "    def __init__(self, pmf_dict):\n",
    "        '''init for PMF\n",
    "        Parameters\n",
    "        -----------------------------------\n",
    "        pmf_dict: dict\n",
    "            dict for probability mass function \n",
    "            with variables as keys and probability as values\n",
    "        '''\n",
    "        assert np.allclose(sum(pmf_dict.values()), 1)\n",
    "        self.outcomes = sorted(pmf_dict.keys())\n",
    "        self.cdf = np.cumsum([pmf_dict[k] for k in self.outcomes])\n",
    "        self.pmf_dict = pmf_dict\n",
    "\n",
    "    def sample(self):\n",
    "        ''' sample from PMF\n",
    "        Return\n",
    "        -------------------------------------\n",
    "        outcomes: str\n",
    "            outcomes of PMF (keys of pmf_dict)\n",
    "        '''\n",
    "        u = np.random.rand()\n",
    "        i = np.argmax(u < self.cdf)\n",
    "        return self.outcomes[i]\n",
    "\n",
    "    @classmethod\n",
    "    def verify(cls):\n",
    "        '''verify empirical mean of samples of a random variable\n",
    "           is close to corresponding probability.\n",
    "        '''\n",
    "        p = PMF({'a':0.5,'b':0.25,'c':0.25})\n",
    "        samples = np.array([p.sample()  for _ in range(10000)])\n",
    "        for k, v in p.pmf_dict.items():\n",
    "            assert np.allclose(np.mean(samples == k), v, atol=5e-2)\n",
    "\n",
    "            \n",
    "PMF.verify()\n",
    "\n",
    "c_rv = PMF({False: 0.7, True: 0.3})\n",
    "print(np.mean([c_rv.sample() == False for _ in range(1000)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textbook Question\n",
    "\n",
    "- Q8.20: Induction on graph structure (recall from MATH1005/6005) (Difficulty $\\star$)\n",
    "- Q8.21: Note typo: it should be $f_s(x_s)$ (Difficulty $\\star\\star$)\n",
    "- Q8.27: Construct example showing greedy method not working (Difficulty $\\star\\star$)\n",
    "- Q8.29: Induction on tree structure (recall from MATH1005/6005) (Difficulty $\\star\\star$)\n",
    "- Extra: Derive eq 8.74 to 8.85 w.r.t Fig 8.51\n",
    "\n",
    "- Q2.44: Manipulation with a more complex conjugate to derive the posterior (Difficulty $\\star\\star$)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
