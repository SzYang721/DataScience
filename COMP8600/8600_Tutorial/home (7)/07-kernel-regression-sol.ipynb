{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17160e6",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fb85b",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca5ae0",
   "metadata": {},
   "source": [
    "In this lab, we will understand the relationship between linear regression and kernel regression by an excercise in predicting the median value of houses in Boston in two ways: using a quadratic basis and using an equivalent kernel function.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Linear regression (week 2)\n",
    "- Kernel methods (week 7 lectures)\n",
    "\n",
    "\n",
    "### Resources\n",
    "- Lecture recording and notes (week 6 lectures)\n",
    "- Bishop, Chapter 6 (Kernel Methods), Introduction and Section 1 (Dual Representations)\n",
    "- Murphy's Machine Learning A Probabilistic Perspective, Chapter 14 (Kernels), 14.2 (Kernel Functions) and 14.4.3 (Kernelised Ridge Regression)\n",
    "\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Applying machine learning techniques with a non-linear basis\n",
    "- Using kernel methods instead of a new basis\n",
    "- Evaluating when using a kernel method would or would not be sensible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8e1c8",
   "metadata": {},
   "source": [
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\dotprod}[2]{\\langle #1, #2 \\rangle}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcda056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fbfeb",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "In this lab, we will use the same [dataset](https://machlearn.gitlab.io/sml2019/tutorials/02-dataset.csv) as in week 2, which describes the price of housing in Boston (see [description](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)). \n",
    "We aim to predict the value of a home from other factors.\n",
    "In this dataset, each row represents the data of one house. The first column is the value of the house, which is the target to predict. The remaining columns are features, which has been normalised to be in the range $[-1, 1]$. The corresponding labels of columns are\n",
    "\n",
    "```'medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'```\n",
    "\n",
    "Download the dataset. Read in the data using ```np.loadtxt``` with the optional argument ```delimiter=','```, as our data is comma separated rather than space separated. Remove the column containing the binary variable ```'chas'```.\n",
    "\n",
    "Check that the data is as expected using ```print()```. It should have 506 rows (examples) and 13 columns (1 label and 12 features). Check that this is the case. \n",
    "\n",
    "Hint: use  assert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd5eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f828e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medv', 'crim', 'zn', 'indus', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
      "[[24.    -1.    -0.64  -0.864 -0.37   0.155  0.283 -0.462 -1.    -0.584\n",
      "  -0.426  1.    -0.821]\n",
      " [21.    -1.    -1.    -0.515 -0.654  0.096  0.565 -0.302 -0.913 -0.79\n",
      "   0.106  1.    -0.591]\n",
      " [34.    -1.    -1.    -0.515 -0.654  0.389  0.199 -0.302 -0.913 -0.79\n",
      "   0.106  0.979 -0.873]\n",
      " [33.    -0.999 -1.    -0.874 -0.7    0.317 -0.116 -0.103 -0.826 -0.866\n",
      "   0.298  0.989 -0.933]\n",
      " [36.    -0.999 -1.    -0.874 -0.7    0.374  0.057 -0.103 -0.826 -0.866\n",
      "   0.298  1.    -0.801]]\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.loadtxt('07-dataset.csv', delimiter=',')\n",
    "\n",
    "# remove chas\n",
    "column_idxes = list(range(len(names)))\n",
    "chas_idx = names.index('chas')\n",
    "wanted_columns = list(column_idxes)\n",
    "wanted_columns.remove(chas_idx)\n",
    "data = loaded_data[:,wanted_columns]\n",
    "data_names = list(names)\n",
    "data_names.remove('chas')\n",
    "\n",
    "print(data_names)\n",
    "print(np.array_str(data[:5], precision=3))\n",
    "assert data.shape == (506,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8ba40",
   "metadata": {},
   "source": [
    "## Part 0: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229208b",
   "metadata": {},
   "source": [
    "We breifly review some concepts from the class: you may also want to look at the Resources list.\n",
    "\n",
    "### Kernel Functions and Basis Functions\n",
    "Kernel functions can be understood as a way to measure the similarity of two objects $x$ and $x'$ in some space $\\mathcal{X}$,\n",
    "$$\n",
    "k: \\mathcal{X} \\times \\mathcal{X} \\to  \\mathbb{R}\n",
    "$$\n",
    "and especially, we mostly wish to consider positive definite kernels, which we will just call kernels. One powerful property of such kernels is that theses are exactly the functions which can be expressed as the inner product of some basis function transformation:\n",
    "$$\n",
    "k: \\mathcal{X} \\times \\mathcal{X} \\to  \\mathbb{R} \\: \\text{is a kernel}  \\iff k(x,x') = \\langle \\phi_{k}(x), \\phi_{k}(x')  \\rangle_{\\mathcal{V}}\n",
    "$$\n",
    "for some basis function $\\phi_k : \\mathcal{X}\\to \\mathcal{V}$, and inner product $\\langle . \\: , \\: \\rangle_\\mathcal{V}$. For our purposes, $\\mathcal{X}$ and $\\mathcal{V}$ may be simply real spaces $\\mathbb{R}^{d}$, $\\mathbb{R}^{d'}$, and $\\langle x,y\\rangle_\\mathbb{R^{d'}}=x^T y$. \n",
    "\n",
    "The important aspect is the correspondence between some kernel function and a basis function. Fix our standard inner product on real spaces as above. Given a kernel, we can find a basis function whose inner product equals the kernel; given a basis function, we can define a kernel via the inner product:\n",
    "$$\n",
    "k \\to \\phi_k \\: s.t. \\: \\phi_k(x)^T \\phi_k(y) = k(x,y)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\phi \\to k_{\\phi} \\: s.t. \\: k_{\\phi}(x,y) :=  \\phi(x)^T \\phi(y)\n",
    "$$\n",
    "\n",
    "In Part 2 of the lab, you will be given a kernel and required to prove that it is expressible via some basis fucntion.\n",
    "\n",
    "#### Quesion 0.1: While all (positive definite) kernels are expressible as an inner product of basis functions in some space, not all are expressible in finite dimensional space. Give an example of a kernel which cannot be expressed via a finite dimensional basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973468e",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "The Guassian kernel,\n",
    "$$\n",
    "k(x,y) = \\exp\\{-\\frac{1}{2}(x-y)^T \\mathbf{\\Sigma}^{-1} (x-y)   \\}\n",
    "$$\n",
    "Is not expressible via any finite dimensional basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231932d",
   "metadata": {},
   "source": [
    "\n",
    "### Kernel Methods\n",
    "#### Quesion 0.2: Explain the basic idea of kernel methods to a classmate. Your answer should cover:\n",
    "- In what sense is a kernel method non-parametric?\n",
    "- In contrast to a parametric model like standard linear regression, how does a kernel method use the training data to eventually make a prediction for a new datapoint? Outline the general structure of both a paramatric model and a kernel model.\n",
    "- In the kernel methods context, what is the Gram matrix $\\mathbf{K}$, and what is an entry in it in terms of the kernel function $k$ and dataset $\\{x_i\\}_{n=1}^{N}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ecadfd",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "\n",
    "The basic idea of kernel methods is to analysis a dataset via similarity between points in that dataset, and make predictions for new datapoints via measures of similarity to the training dataset. \n",
    "\n",
    "An important benefit of this is that since we concern ourself just with the information of this similarity betwen datapoints, we can use arbitaryily high-dimensional (implicit) feature spaces, since we never actually need to produce $\\phi(x)$, only $k(x,x')$.\n",
    "\n",
    "A parametric model is a family of models characterised by their parameters $\\{f_{\\theta} \\:  : \\: \\theta \\in \\Theta \\}$, which makes then predictions $f_\\theta(x) = f(x \\: | \\: \\theta)$ for a new point $x$. We choose amongst the family a model (a paramater settting) which performs well on some dataset $\\{x_i, y_i\\}_i$.\n",
    "\n",
    "A kernel method is not reducible to a paramater family: kernel methods use the training data in every prediction. Intuitively, kernel methods use their kernel function to measure the similarity between datapoints, including a new datapoint $x'$ to the training data $\\{ x_i\\}_i$, to make predictions. While kernel methods do have variables which can be set to optimal values (as you'll derive in Question 0.3), making a new prediction relies on $k(x',x_i)$ for the training data, and so the model cannot be characterised by its internal variables.\n",
    "\n",
    "The Gram matrix measures similarity between different datapoints in the training dataset. An entry of $\\mathbf{K}$ is\n",
    "$$\n",
    "\\mathbf{K}_{i,j} = k(x_i, x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a15d72e",
   "metadata": {},
   "source": [
    "### Kernel Methods, Linear Regression and Dual Representation\n",
    "This lab compares a familiar technique, regularised linear regression with a basis function, to the method of kernel regression. \n",
    "\n",
    "We recall that our linear regression optimises the parameter $\\mathbf{w}$ of our model $f_{w}(x')=\\mathbf{w}^T \\phi(x')$ on transformed datapoints $x'$ best minimises the regularised sum-of-squares loss\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N(\\mathbf{w}^T \\phi(\\mathbf{x_n}) - t_n)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T\\mathbf{w}, \\:\\:\\:\\:(A)$$\n",
    "Now, ultimately we will perform kernel regression by a function\n",
    "$$\n",
    "f(x') = \\mathbf{k}(x')^T (\\mathbf{K} + \\lambda \\mathbf{I})^{-1} \\mathbf{t}\n",
    "$$\n",
    "where $\\mathbf{k}(x')$ is the vector of kernel outputs $k(x', x_i)$ for each $x_i$ in the training data. First however, we wish to derive an expression for the loss as a function of $\\mathbf{a}$ in terms of $\\mathbf{a}$ and the Gram matrix $\\mathbf{K}$:\n",
    "$$J(\\mathbf{a}) = \\frac{1}{2} \\mathbf{a}^T \\mathbf{KKa} - \\mathbf{a}^T \\mathbf{Kt} + \\frac{1}{2}\\mathbf{t}^T \\mathbf{t} + \\frac{\\lambda}{2} \\mathbf{a}^T\\mathbf{Ka} , \\:\\:\\:\\:(B)$$\n",
    "where $\\mathbf{K=\\Phi \\Phi}^T$ with elements $K_{nm} = \\phi(\\mathbf{x_n})^T\\phi(\\mathbf{x_m})$, and $\\mathbf{a}$ is the vector with entries $a_n = -\\frac{1}{\\lambda} (\\mathbf{w}^T \\phi(x_n)-t_n)$ for each $(x_n, t_n)$ in the training data.\n",
    "\n",
    "#### Question 0.3: Derive (B) from (A)  (don't spend too long on this, you may leave it and continue with the lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5b22d",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "See Bishop 6.1 \"Dual Representation\" (page number 293 or pdf page 313).\n",
    "https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fe3c7",
   "metadata": {},
   "source": [
    "## Overview of Lab\n",
    "\n",
    "### Part 0.5: Linear Regression Review\n",
    "We breifly review linear regression with basis functions, and you implement code to find the analytic solution to $\\mathbf{w}$\n",
    "\n",
    "### Part 1: Linear Regression with Quadratic Basis Function\n",
    "Implement a quadratic basis function $\\phi(x)$ and perform linear regression.\n",
    "\n",
    "### Part 2: Linear Regression with a Corresponding Kernel\n",
    "Show that a given kernel function $k(x,y)$ is equivalent to the quadratic basis in Part 1. Implement a calculation of $k(x,y)$ across datasets via matrix operations, and perform kernelised linear regression.\n",
    "\n",
    "### Part 3: Comparison and Reflection\n",
    "Discuss the above linear regression and kernelised linear regression.\n",
    "\n",
    "### Part 3.5: Other Kernels (Optional)\n",
    "Implement a new kernel $k'(x,y)$ of your choice, and see it used in kernelised linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58c01d",
   "metadata": {},
   "source": [
    "## Part 0.5: Linear Regression Review\n",
    "\n",
    "First remind yourself of what linear regression is by our implemention of linear regression with regularisation on the Boston dataset. Use 80% of the available data for training the model using maximum likelihood with regularisation (assuming Gaussian noise). The rest of the data is allocated to the test set.\n",
    "\n",
    "Report the root mean squared error (RMSE) for the training set and the test set. We'll compare the results with the linear regression with basis function, and the linear regression with kernel function later on.\n",
    "\n",
    "**TODO**: Implement the analytic solution of $\\frac{\\partial J(\\mathbf{w})}{\\partial\\mathbf{w}}=0$ in the function  ```w_ml_regularised(Phi, t, l)```, where ```l``` stands for $\\lambda$. Note that $$\\Phi=\n",
    "\\begin{pmatrix}\n",
    "    \\phi(\\mathbf{x_1})^T \\\\\n",
    "    \\phi(\\mathbf{x_2})^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\phi(\\mathbf{x_n})^T \\\\\n",
    "\\end{pmatrix},$$\n",
    "where $\\phi(\\mathbf{x_i})$ denotes the feature vector for the $i$-th datapoint, and $n$ denotes the total number of datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b62dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression: RMSE with regularisation: Train 4.741045, Test 4.891587\n",
      "Expected value: Train 4.74, Test 4.89\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def w_ml_regularised(Phi, t, l):\n",
    "    \"\"\"Produce the analytic solution of w given input features and labels.\"\"\"\n",
    "\n",
    "    return np.linalg.inv(l * np.eye(Phi.shape[1]) + Phi.T @ Phi) @ Phi.T @ t\n",
    "\n",
    "def split_data(data, train_size):\n",
    "    \"\"\"Randomly split data into two groups. The first group is a fifth of the data.\"\"\"\n",
    "    np.random.seed(1)\n",
    "    N = len(data)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:int(train_size * N)]\n",
    "    test_idx = idx[int(train_size * N):]\n",
    "\n",
    "    # Assume label is in the first column\n",
    "    X_train = data[train_idx, 1:]\n",
    "    t_train = data[train_idx, 0]\n",
    "    X_test = data[test_idx, 1:]\n",
    "    t_test = data[test_idx, 0]\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "def rmse(X_train, t_train, X_test, t_test, w):\n",
    "    \"\"\"Return the RMSE for training and test sets\"\"\"\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "\n",
    "    # Training set error\n",
    "    t_train_pred = np.dot(X_train, w)\n",
    "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
    "\n",
    "    # Test set error\n",
    "    t_test_pred = np.dot(X_test, w)\n",
    "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
    "\n",
    "    return rmse_train, rmse_test\n",
    "\n",
    "def lr(X, l, split_rate):\n",
    "    \"\"\"Return RMSE for the training set and the test set\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------\n",
    "    X: numpy matrix, whole dataset \n",
    "    l: float, regularisation parameter\n",
    "    split_rate: int, the percent of training dataset\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------\n",
    "    train_rmse: float, RMSE for training set\n",
    "    test_rmse: float, RMSE for testing set\n",
    "    \"\"\"\n",
    "    X0 = np.ones((data.shape[0], 1))\n",
    "    X = np.hstack((X, X0))\n",
    "\n",
    "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
    "    w_reg = w_ml_regularised(X_train, t_train,l)\n",
    "\n",
    "    train_rmse, test_rmse = rmse(X_train, t_train, X_test, t_test, w_reg)\n",
    "    return train_rmse, test_rmse\n",
    "\n",
    "train_rmse, test_rmse = lr(data, 1.1, 0.8)\n",
    "print(\"Regression: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print(\"Expected value: Train 4.74, Test 4.89\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d30299",
   "metadata": {},
   "source": [
    "## Part 1: Linear regression with quadratic basis function\n",
    "\n",
    "Let $X \\in \\RR^{n \\times d}$ be the data matrix containing n datapoints $\\mathbf{x_i} \\in \\RR^{d}$. We can choose to train and test using the raw data $X$ as the input to our model as the above method. Alternatively, we could use $$\\Phi= [\\mathbf{\\phi(x_1)}, \\mathbf{\\phi(x_2)}, ..., \\mathbf{\\phi(x_n)}]^T \\in \\RR^{n \\times m},$$ where $\\mathbf{\\phi(x_i)}$ is some transformation of $\\mathbf{x_i}$, m is the dimension of $\\mathbf{\\phi(x_i)}$.\n",
    "\n",
    "For this lab, write $\\mathbf{x_i} = (x_{i,1},\\dots,x_{i,d})$. Let\n",
    "$$\n",
    "\\phi(\\mathbf{x_i}) = (x_{i,1}^2, x_{i,2}^2, \\ldots, x_{i,d}^2, \\sqrt{2}x_{i,1} x_{i,2}, \\ldots, \\sqrt{2}x_{i,d-1}x_{i,d}, \\sqrt{2}x_{i,1}, \\ldots, \\sqrt{2}x_{i,d}, 1).\n",
    "$$\n",
    "Note that $\\phi(\\mathbf{x_i})$ is all quadratic functions of elements of $\\mathbf{x_i}$ and 1 (The $\\sqrt{2}$ coefficients are for normalisation for later in the lab).\n",
    "We say that we are using a *quadratic basis function*.\n",
    "\n",
    "Train and test the data with ```quadratic_lr```, report the RMSE for the training set and the test set.\n",
    "\n",
    "**TODO**:  \n",
    "1. write a function called ```phi_quadratic``` with a single datapoint $\\mathbf{x_i}$ as input and the quadratic basis function $\\phi(\\mathbf{x_i})$ as output.  \n",
    "2. write a function called ```feature_map``` with raw data matrix $X$ as input and $\\Phi$ as output by using ```phi_quadratic``` for each datapoint. \n",
    "3. in function ```quadratic_lr```, make use of previous functions and give the analytic solution for $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b85968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic basis: RMSE with regularisation: Train 2.790632, Test 3.339125\n",
      "Expected value: Train 2.79, Test 3.34\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def phi_quadratic(x):\n",
    "    \"\"\"Compute phi(x) for a single training example using quadratic basis function.\"\"\"\n",
    "    D = len(x)\n",
    "    # Features are (1, {x_i}, {cross terms and squared terms})\n",
    "    # Cross terms x_i x_j and squared terms x_i^2 can be taken from upper triangle of outer product of x with itself\n",
    "    utri = np.sqrt(2)*np.hstack((x, np.outer(x, x)[np.triu_indices(D, k=1)]))\n",
    "    diag = x * x\n",
    "    res = np.hstack((1,diag, utri))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def feature_map(X):\n",
    "    \"\"\"Return the matrix of the feature map.\"\"\"\n",
    "    num_ex, num_feat = X.shape\n",
    "    D = int(1+num_feat+num_feat*(num_feat+1)/2)\n",
    "    Phi = np.zeros((num_ex, D))\n",
    "    np.shape(Phi)\n",
    "    for ix in range(num_ex):\n",
    "        Phi[ix,:] = phi_quadratic(X[ix,:])\n",
    "    return Phi\n",
    "\n",
    "def quadratic_lr(X, l, split_rate):\n",
    "    \"\"\"Return RMSE for the training set and the test set\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------\n",
    "    X: numpy matrix, whole dataset \n",
    "    l: float, regularisation parameter\n",
    "    split_rate: int, the percent of training dataset\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------\n",
    "    train_rmse: float, RMSE for training set\n",
    "    test_rmse: float, RMSE for testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
    "    # alternatively: use train_test_split\n",
    "    # X_train, X_test, t_train, t_test = train_test_split(X[:,1:], X[:, 0], test_size = 1-split_rate, random_state = 42)\n",
    "    w_reg = w_ml_regularised(feature_map(X_train), t_train,l)\n",
    "\n",
    "    train_rmse, test_rmse = rmse(feature_map(X_train), t_train, feature_map(X_test), t_test, w_reg)\n",
    "    return train_rmse, test_rmse\n",
    "\n",
    "train_rmse, test_rmse = quadratic_lr(data, 1.1, 0.8)\n",
    "print(\"Quadratic basis: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print(\"Expected value: Train 2.79, Test 3.34\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89bdb9",
   "metadata": {},
   "source": [
    "## Part 2: Linear regression with a  Corresponding Kernel\n",
    "\n",
    "### Computing a basis transformation as an inner product\n",
    "\n",
    "Define $k(\\mathbf{x,y}) = (\\dotprod{\\mathbf{x}}{\\mathbf{y}} + 1)^2$, where $\\mathbf{x,y} \\in \\mathbb{R}^2$. One way to verify $k(\\mathbf{x,y})$ is a kernel function is to write this as an inner product of a verctor valued function evaluated at $\\mathbf{x}$ and $\\mathbf{y}$. That is, show we have $k(\\mathbf{x}, \\mathbf{y}) = \\dotprod{\\phi(\\mathbf{x})}{\\phi(\\mathbf{y})}$ and specify what is $\\phi(\\mathbf{x}), \\phi(\\mathbf{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510a3a5",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\\begin{align}\n",
    "k(\\mathbf{x,y}) &= (\\dotprod{\\mathbf{x}}{\\mathbf{y}} + 1)^2 = (\\mathbf{x}^T\\mathbf{y}+ 1)^2 = (x_1y_1 + x_2y_2 + 1)^2\\\\\n",
    "&= x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2 + 2x_1y_1 + 2x_2y_2 + 1\\\\\n",
    "&= (x_1^2, x_2^2, \\sqrt{2} x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2, 1)(y_1^2, y_2^2, \\sqrt{2} y_1y_2, \\sqrt{2}y_1, \\sqrt{2}y_2, 1)^T\n",
    "\\end{align}\n",
    "Let $\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2} x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2, 1)^T$, $\\phi(\\mathbf{y}) = (y_1^2, y_2^2, \\sqrt{2} y_1y_2, \\sqrt{2}y_1, \\sqrt{2}y_2, 1)^T$, then we have $k(\\mathbf{x,y}) = \\phi(\\mathbf{x})^T\\phi(\\mathbf{y}) = \\dotprod{\\phi(\\mathbf{x})}{\\phi(\\mathbf{y})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a510f",
   "metadata": {},
   "source": [
    "Then convince yourself that, for $X \\in \\RR^{n \\times d}, Y \\in \\RR^{m \\times d}$, then $K = (X Y^T  + 1)^2 \\in \\RR^{n \\times m}$ (addition and square term are element-wise) contains elements as kernel function between each pair of datapoints of X and Y,\n",
    "$$K_{ij} = \\phi(\\mathbf{x_i})^T \\phi(\\mathbf{y_j}) = k(\\mathbf{x_i}, \\mathbf{y_j}).$$\n",
    "\n",
    "### Kernelised Regression\n",
    "**TODO**:\n",
    "Write the function ```kernel_quadratic``` which takes $X$, $Y$  as input and $K$ as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f028971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def kernel_quadratic(X, Y):\n",
    "    dot_product = np.dot(X, Y.T)\n",
    "    return (dot_product+1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3054515",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "Complete the function ```kernelised_lr``` with raw data matrix $X$ as input and root mean squared error (RMSE) for the training set and the test set as output. Inside of the function, make use of ```kernel_quadratic``` to apply dual representation, and calculate the predicted labels for training data and testing data repectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18467c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernelised Regression: RMSE with regularisation: Train 2.790632, Test 3.339125\n",
      "Expected value: Train 2.79, Test 3.33\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def kernelised_lr(X, l, split_rate):\n",
    "    \"\"\"Return RMSE for the training set and the test set\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------\n",
    "    X: numpy matrix, whole dataset \n",
    "    l: float, regularisation parameter\n",
    "    split_rate: int, the percent of training dataset\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------\n",
    "    rmse_train: float, RMSE for training set\n",
    "    rmse_test: float, RMSE for testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
    "    N_train = X_train.shape[0]\n",
    "    N_test = X_test.shape[0]\n",
    "    \n",
    "    K_train = kernel_quadratic(X_train, X_train)\n",
    "    K_test = kernel_quadratic(X_test, X_train)\n",
    "    \n",
    "    # critical points\n",
    "    a =  np.linalg.inv(K_train + l * np.eye(N_train)).dot(t_train)\n",
    "    \n",
    "    # Training set error\n",
    "    t_train_pred = np.dot(K_train, a)\n",
    "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
    "\n",
    "    # Test set error\n",
    "    t_test_pred = np.dot(K_test, a)\n",
    "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
    "\n",
    "    return rmse_train, rmse_test\n",
    "\n",
    "train_rmse, test_rmse = kernelised_lr(data, 1.1, 0.8)\n",
    "print(\"Kernelised Regression: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print(\"Expected value: Train 2.79, Test 3.33\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9bfaee",
   "metadata": {},
   "source": [
    "## Part 3: Comparison and Reflection\n",
    "\n",
    "### Time complexity\n",
    "\n",
    "Compare the above two methods (quadratic basis function, kernel method) in terms of time complexity.\n",
    "\n",
    "In terms of time complexity, is using a kernel method suitable in this case? What are potential advantages of using a kernel method? Disadvantages? \n",
    "\n",
    "### Representation of the Feature Space\n",
    "\n",
    "In this example, the size of the feature space was quadratic in the number of original features of the dataset. Consider the memory cost of representing the transformed data. Give an example of a kernel with a basis function that should not be attempted to be stored in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21681617",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Assume the dataset with feature mapping is $N \\times M$. \n",
    "\n",
    "The linear regression with quadratic basis function requires to invert an $M \\times M$ matrix, while the linear regression with kernel method requires to invert a $N \\times N$ matrix. In our case, the feature mapping space is 91 (which is 12 + (12 * 11)/2 + 12 + 1), which is much smaller than the number of data points (506 in total). Thus a kernel method might not suit our case in terms of time complexity, but when N is smaller than M, kernel method is more efficient.\n",
    "\n",
    "\n",
    "The Guassian kernel should not have it's corresponding basis features represented, because they are infinite dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1011198",
   "metadata": {},
   "source": [
    "## Part 3.5: Other Kernels (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca6330",
   "metadata": {},
   "source": [
    "We have seen how to implement kerenlised regression for the quadratic basis function. There are of course many other kernels, such as \n",
    "- Guassian kernels\n",
    "- Linear kernels\n",
    "- \n",
    "Choose from the above or another\n",
    "\n",
    "Depending on the kernel function you choose, you may find it easier to implement the function itself (k(x,y)) and use it in your implementation of \"your_kernel\", or you may be able to directly compute \"your_kernel(X,Y)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1a2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_kernel_function(x, y):\n",
    "    z = x - y\n",
    "    k = np.exp(-0.5 * np.dot(z,z))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e043c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_kernel(X, Y):\n",
    "    \n",
    "    N_X = X.shape[0]\n",
    "    N_Y = Y.shape[0]\n",
    "    \n",
    "    K = np.zeros((N_X, N_Y))\n",
    "    \n",
    "    for i in range(N_X):\n",
    "        for j in range(N_Y):\n",
    "            x_i = X[i]\n",
    "            y_j = Y[j]\n",
    "            K[i,j] = your_kernel_function(x_i, y_j)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83748faf",
   "metadata": {},
   "source": [
    "Now for you to reimplement kernelised linear regression: your code here should not differ significantly from that which you used in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db44879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernelised Regression: RMSE with regularisation: Train 3.743547, Test 4.209796\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def your_kernelised_lr(X, l, split_rate):\n",
    "    \"\"\"Return RMSE for the training set and the test set\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------\n",
    "    X: numpy matrix, whole dataset \n",
    "    l: float, regularisation parameter\n",
    "    split_rate: int, the percent of training dataset\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------\n",
    "    rmse_train: float, RMSE for training set\n",
    "    rmse_test: float, RMSE for testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
    "    N_train = X_train.shape[0]\n",
    "    N_test = X_test.shape[0]\n",
    "    \n",
    "    K_train = your_kernel(X_train, X_train)\n",
    "    K_test = your_kernel(X_test, X_train)\n",
    "    \n",
    "    # critical points\n",
    "    a =  np.linalg.inv(K_train + l * np.eye(N_train)).dot(t_train)\n",
    "    \n",
    "    # Training set error\n",
    "    t_train_pred = np.dot(K_train, a)\n",
    "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
    "\n",
    "    # Test set error\n",
    "    t_test_pred = np.dot(K_test, a)\n",
    "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
    "\n",
    "    return rmse_train, rmse_test\n",
    "\n",
    "train_rmse, test_rmse = your_kernelised_lr(data, 1.1, 0.8)\n",
    "print(\"Kernelised Regression: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7fc94",
   "metadata": {},
   "source": [
    "Why did you choose the kernel function that you did? How did it perform relative to the quadratic kernel? In what situation (type of dataset, properties of the data or distribution) might your chosen kernel function be advantagous compared to the quadratic kernel? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645da22",
   "metadata": {},
   "source": [
    "## Textbook Questions (Optional)\n",
    "These questions are hand picked to both be of reasonable difficulty and demonstrate what you are expected to be able to solve. The questions are labelled in Bishop as either $\\star$, $\\star\\star$, or $\\star\\star\\star$ to rate its difficulty.\n",
    "\n",
    "- **Question 6.1**: Understand dual formulation (Difficulty $\\star\\star$, simple algebraic derivation)\n",
    "- **Question 6.2**: Dual formulation for perceptron learning algorithm (Difficulty $\\star\\star$, simple algebraic derivation)\n",
    "- **Question 6.11**: You may want to use Taylor expansion to represent term $\\exp(\\mathbf{x}^T\\mathbf{x}'/2\\sigma^2)$ (Difficulty $\\star$)\n",
    "- **Question 6.12**: To prove $k\\left(A_{1}, A_{2}\\right)=2^{\\left|A_{1} \\cap A_{2}\\right|}=\\phi\\left(A_{1}\\right)^{T} \\phi\\left(A_{2}\\right)$. (Difficulty $\\star\\star$)\n",
    "- **Question 6.13**: Use chain rule to represent $g(\\varphi(\\theta), x)$ (Difficulty $\\star$)\n",
    "- **Question 7.6**: (Difficulty $\\star$, simple algebraic derivation)\n",
    "- **Question 7.7**: (Difficulty $\\star$, simple algebraic derivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151286e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
