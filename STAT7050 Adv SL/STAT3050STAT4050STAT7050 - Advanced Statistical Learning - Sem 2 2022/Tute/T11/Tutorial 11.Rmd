---
title: "R Notebook on Bagging and Random Forest"
output:
  html_document:
    df_print: paged
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
# Bagging
# for data wrangling
library(dplyr) 
# for calculating variable importance
library(e1071)
# for general model fitting
library(caret)
# for fitting decision trees
library(rpart)
# for fitting bagged decision trees
library(ipred)
# The dataset airquality is a built-in R dataset which contains air quality measurements in New York on 153 individual days. 
str(airquality)
# The bagging() function from the ipred library is applied on airquality dataset.
# 150 bootstrapped samples are generated to build the bagged model and the coob to be TRUE to obtain the estiamted out-of-bag error. 
# minsplit=2 tells the model to only require 2 observations in a node to split. cp is the complexity parameter. By setting it to zero, we do not require the model to be able to improve the overall fit by any amount in order to perform a split. 
set.seed(1)
bag=bagging(formula=Ozone~., data=airquality, nbagg=150, coob=TRUE, control=rpart.control(minsplit = 2, cp=0))
bag
# visualize the importance of the predictor variables by calculating the total reduction in RSS (residual sum of squares) due to the split over a given predictor, averaged over all of the trees. 
# the varlmp() function from the caret library will create a variable importance plot for the fitted bagged model.
# calculate variable imporance
VI=data.frame(var=names(airquality[, -1]), imp=varImp(bag))
# sort variable importance descending
VI_plot=VI[order(VI$Overall, decreasing = TRUE), ]
# visualize variable importance with horizontal bar plot
barplot(VI_plot$Overall, names.arg = rownames(VI_plot), horiz = TRUE, col='steelblue', xlab='Variable Importance')
# Use the Bagging Model to make predictions
# define new observation
new=data.frame(Solar.R=150, Wind=8, Temp=70, Month=5, Day=5)
# use fitted bagged model to predict Ozone value for new observation
predict(bag, newdata = new)
```
```{r}
# Random Forest
# the randomForest package in R can do bagging and random forests. Recall that bagging is simply a special case of a random forest with m=p. Therefore, the randomForest() function can be used to perform both random forests and bagging. 
library(randomForest)
# Fit a regression tree to the Boston dataset. First, we create a training set, and fit the tree to the training data. 
library(MASS)
set.seed(1)
train=sample(1:nrow(Boston), nrow(Boston)/2)
# We perform bagging as follows. The argument mtry=12 indicates that all 12 predictors should be considered for each split of the tree. 
bag.boston=randomForest(medv~., data=Boston, subset=train, mtry=12, importance=TRUE)
bag.boston
# prediction from bagging
yhat.bag=predict(bag.boston, newdata = Boston[-train, ])
boston.test=Boston[-train, "medv"]
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag-boston.test)^2)
# We could change the number of trees grown by randomForest() using the ntree argument.
bag.boston=randomForest(medv~., data=Boston, subset=train, mtry=12, ntree=25)
yhat.bag=predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag-boston.test)^2)
# Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses p/3 variables when building a random forest of regression trees, and p^{1/2} variables when building a random forest of classification trees.
set.seed(1)
rf.boston=randomForest(medv~., data=Boston, subset=train, mtry=6, importance=TRUE)
yhat.rf=predict(rf.boston, newdata=Boston[-train, ])
mean((yhat.rf-boston.test)^2)
# the importance() function can view the importance of each variable
# Two measures of variable importance are reported. The first is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is permuted. The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees. In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. 
importance(rf.boston)
# Plots of these importance measures can be produced by the varImpPlot() function.
varImpPlot(rf.boston)
```




Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

