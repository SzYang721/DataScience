---
title: "R Notebook for Week 2"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

## Elastic Net

```{r}
# Quick Start
# We use the Hitters dataset from the ISLR package to explore the shrinkage method, elastic net. 
data(Hitters, package = "ISLR")
# We again remove the missing data, which was all in the response variable, Salary.
Hitters = na.omit(Hitters)
tibble::as_tibble(Hitters)
dim(Hitters)
library(caret)
library(glmnet)
histogram(Hitters$Salary, xlab = "Salary, $1000s", 
          main = "Baseball Salaries, 1986 - 1987")
set.seed(42)

#The aim to estimate a regression model y=Xb+error component by the elastic net function glmnet.
X = model.matrix(Salary ~ . ^ 2, Hitters)[, -1]
y = Hitters$Salary
fit <- glmnet(X, y)#this is just lasso
# fit is an object of class glmnet that contains all the relevant information of the fitted model for further use. Various methods are provided for the object such as plot, print, coef and predict that enable us to execute those tasks more elegantly.

# We can visualize the coefficients by executing the plot method:
plot(fit)
# Each curve corresponds to a variable. It shows the path of its coefficient against the ell_1 norm of the whole coefficient vector as \lambda varies. The axis above indicates the number of nonzero coefficients at the current lambda, which is the effective degrees of freedom (df) for the lasso. Users may also wish to annotate the curves: this can be done by setting label = TRUE in the plot command.

# A summary of the glmnet path at each step is displayed if we just enter the object name or use the print function:
print(fit)
# It shows from left to right the number of nonzero coefficients (Df), the percent (of null) deviance explained (%dev) and the value of Lambda. Although glmnet fits the model for 100 values of lambda by default, it stops early if %dev does not change sufficently from one lambda to the next (typically near the end of the path.) Here we have truncated the prinout for brevity.

# We can obtain the model coefficients at one or more lambda's within the range of the sequence:
coef(fit, s = 0.1)# s is the lambda as in the model
# (Why s and not lambda? In case we want to allow one to specify the model size in other ways in the future.) Users can also make predictions at specific lambda's with new input data:
set.seed(29)
nx <- matrix(rnorm(1*190), 1, 190)
predict(fit, newx = nx, s = c(0.1, 0.05))

# The function glmnet returns a sequence of models for the users to choose from. In many cases, users may prefer the software to select one of them. Cross-validation is perhaps the simplest and most widely used method for that task. cv.glmnet is the main function to do cross-validation here, along with various supporting methods such as plotting and prediction.
cvfit <- cv.glmnet(X, y)
# cv.glmnet returns a cv.glmnet object, a list with all the ingredients of the cross-validated fit. As with glmnet, we do not encourage users to extract the components directly except for viewing the selected values of lambda. The package provides well-designed functions for potential tasks. For example, we can plot the object:
plot(cvfit)
# This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the lambda sequence (error bars). Two special values along the lambda sequence are indicated by the vertical dotted lines. lambda.min is the value of lambda that gives minimum mean cross-validated error, while lambda.1se is the value of lambda that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.

# We can use the following code to get the value of lambda.min and the model coefficients at that value of lambda:
cvfit$lambda.min
coef(cvfit, s = "lambda.min")
#To get the corresponding values at lambda.1se, simply replace lambda.min with lambda.1se above, or omit the s argument, since lambda.1se is the default.

#Note that the coefficients are represented in the sparse matrix format. This is because the solutions along the regularization path are often sparse, and hence it is more efficient in time and space to use a sparse format. If you prefer non-sparse format, pipe the output through as.matrix().

#Predictions can be made based on the fitted cv.glmnet object as well. The code below gives predictions for the new input matrix newx at lambda.min:
predict(cvfit, newx = X[1:5,], s = "lambda.min")
# With the tools introduced so far, users are able to fit the entire elastic net family, including ridge regression, using squared-error loss. There are many more arguments in the package that give users a great deal of flexibility. To learn more, move on to later sections.
```


```{r}
# more details
# Commonly used function arguments
# glmnet provides various arguments for users to customize the fit: we introduce some commonly used arguments here. (For more information, type ?glmnet.)

# alpha is for the elastic net mixing parameter apha, with range alpha\in [0,1]. ð›¼=1 is lasso regression (default) and alpha=0 is ridge regression.

# weights is for the observation weights, default is 1 for each observation. (Note: glmnet rescales the weights internally to sum to N, the sample size.)

# nlambda is the number of lambda values in the sequence (default is 100).

# lambda can be provided if the user wants to specify the lambda sequence, but typical usage is for the program to construct the lambda sequence on its own. When automatically generated, the lambda sequence is determined by lambda.max and lambda.min.ratio. The latter is the ratio of smallest value of the generated lambda sequence (say lambda.min) to lambda.max. The program generates nlambda values linear on the log scale from lambda.max down to lambda.min. lambda.max is not user-specified but is computed from the input x and y: it is the smallest value for lambda such that all the coefficients are zero. For alpha = 0 (ridge) lambda.max would be infty: in this case we pick a value corresponding to a small value for alpha close to zero.)

# standardize is a logical flag for x variable standardization prior to fitting the model sequence. The coefficients are always returned on the original scale. Default is standardize = TRUE.

#As an example, we set alpha=0.2 (more like a ridge regression), and give double weight to the latter half of the observations. We set nlambda to 20 so that the model fit is only compute for 20 values of lambda. In practice, we recommend nlambda to be 100 (default) or more. In most cases, it does not come with extra cost because of the warm-starts used in the algorithm, and for nonlinear models leads to better convergence properties.
wts <-  c(rep(1,100), rep(2,163))
fit <- glmnet(X, y, alpha = 0.2, weights = wts, nlambda = 20)

#We can extract the coefficients and make predictions for a glmnet object at certain values of lambda. One commonly used argument is:
# s for specifiying the value(s) of lambda at which to extract coefficients/predictions.

# Here is a simple example illustrating the use of both these function arguments:
fit <- glmnet(X, y)
any(fit$lambda == 0.5)  # 0.5 not in original lambda sequence

coef.apprx <- coef(fit, s = 0.5)

# We can plot the fitted object as in the Quick Start section. Here we walk through more arguments for the plot function. The xvar argument allows users to decide what is plotted on the x-axis. xvar allows three measures: â€œnormâ€ for the ell_1 norm of the coefficients (default), â€œlambdaâ€ for the log-lambda value and â€œdevâ€ for %deviance explained. Users can also label the curves with the variable index numbers simply by setting label = TRUE.

# For example, letâ€™s plot fit against the log-lambda value and with each curve labeled:
plot(fit, xvar = "lambda", label = TRUE)

# Now when we plot against %deviance we get a very different picture. This is percent deviance explained on the training data, and is a measure of complexity of the model. We see that toward the end of the path, %deviance is not changing much but the coefficients are â€œblowing upâ€ a bit. This enables us focus attention on the parts of the fit that matter. This will especially be true for other models, such as logistic regression.
plot(fit, xvar = "dev", label = TRUE)
```

```{r}
# K-fold cross-validation can be performed using the cv.glmnet function. In addition to all the glmnet parameters, cv.glmnet has its special parameters including nfolds (the number of folds), foldid (user-supplied folds), and type.measure(the loss used for cross-validation):
# â€œdevianceâ€ or â€œmseâ€ for squared loss, and
# â€œmaeâ€ uses mean absolute error.
# As an example,
cvfit <- cv.glmnet(X, y, type.measure = "mse", nfolds = 20)
# does 20-fold cross-validation based on mean squared error criterion (the default for â€œgaussianâ€ family). Printing the resulting object gives some basic information on the cross-validation performed:
print(cvfit)

# Users can explicitly control the fold that each observation is assigned to via the foldid argument. This is useful, for example, in using cross-validation to select a value for alpha:
foldid <- sample(1:10, size = length(y), replace = TRUE)
cv1  <- cv.glmnet(X, y, foldid = foldid, alpha = 1)
cv.5 <- cv.glmnet(X, y, foldid = foldid, alpha = 0.5)
cv0  <- cv.glmnet(X, y, foldid = foldid, alpha = 0)

# There are no built-in plot functions to put them all on the same plot, so we are on our own here:

par(mfrow = c(2,2))
plot(cv1); plot(cv.5); plot(cv0)
plot(log(cv1$lambda)   , cv1$cvm , pch = 19, col = "red",
     xlab = "log(Lambda)", ylab = cv1$name)
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "grey")
points(log(cv0$lambda) , cv0$cvm , pch = 19, col = "blue")
legend("topleft", legend = c("alpha= 1", "alpha= .5", "alpha 0"),
       pch = 19, col = c("red","grey","blue"))
```




