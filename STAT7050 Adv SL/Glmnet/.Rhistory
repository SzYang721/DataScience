install.packages("caret")
install.packages("caret")
library(PerformanceAnalytics)
library(mice)
library(corrplot)
library(faraway)
library(MASS)
library(leaps)
library(reshape2)
library(tidyverse)
library(ggplot2)
library(GGally)
library(ISLR)
library(corrplot)
library(Hmisc)
library(dplyr)
library(VIM)
library(lsr)
library (glmnet)
library (pls)
library(tree)
library(randomForest)
library (gbm)
library (BART)
library(keras)
library(tensorflow)
library(xgboost)
library(Matrix)
library(caret)
library(caret)
pacman::p_install("caret")
library(caret)
pacman::p_load(caret)
library(caret)
remove.packages(caret)
remove.packages("caret)
remove.packages("caret")
remove.packages("caret")
library(caret)
pacman::p_load(caret)
train_control<-trainControl(method = "cv",number = 5, search = "grid")
?trainControl
gbmGrid<-expand.grid(max_depth(c(3:7)),
nrounds = (1:10)*50,
#default value
eta = 0.3,
gamma = 0,
subsample = 1,
min_chlid_weight=1,
colsample_bytree = 0.6)
gbmGrid<-expand.grid(max_depth=c(3:7),
nrounds = (1:10)*50,
#default value
eta = 0.3,
gamma = 0,
subsample = 1,
min_chlid_weight=1,
colsample_bytree = 0.6)
library(caret)
detach("package:caret", unload = TRUE)
library(caret)
train_control<-trainControl(method = "cv",number = 5, search = "grid")
install.packages('caret')
library(caret)
install.packages("hardhat")
install.packages("hardhat")
library(hardhat)
knitr::opts_chunk$set(echo = TRUE)
library(PerformanceAnalytics)
library(mice)
library(corrplot)
library(faraway)
library(MASS)
library(leaps)
library(reshape2)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(GGally)
library(corrplot)
library(ISLR)
library(dplyr)
library(VIM)
library(lsr)
library (glmnet)
library (pls)
library(tree)
library(randomForest)
library (gbm)
library (BART)
library(keras)
library(tensorflow)
library(xgboost)
library(Matrix)
library(caret)
library(class)
setwd("E:/ANU Sem 2/STAT3040STAT7040 - Statistical Learning/A Final Project")
train <-
read.csv(file="train.csv",header=TRUE)
test<-read.csv(file="test.csv",header=TRUE)
plot(best.mod, type = "both")
plot(best.mod, type = "both")
best.mod<-lda(mod.final,data = train.imp)
mod.final<-as.formula(sent ~  + sq.m.h + built + bedrooms + sq.m.pool)
best.mod<-lda(mod.final,data = train.imp)
plot(best.mod, type = "both")
?abline()
abline(h = best.mod$means)
abline(h = best.mod$means)
plot(best.mod, type = "both")
abline(h = best.mod$means)
abline(v = best.mod$means)
plot(best.mod, type = "both")
abline(v = best.mod$means)
abline(v = best.mod$means)
mod.final<-as.formula(sent ~  + sq.m.h + built + bedrooms + sq.m.pool)
best.mod<-lda(mod.final,data = train.imp)
plot(best.mod, type = "both")
abline(v = best.mod$means)
abline(v = best.mod$means)
best.mod$means
best.mod$prior
summary(best.mod)
?lda()
best.mod
best.mod$means
plot(best.mod, type = "both")
best.mod$means[1,]
mean(best.mod$means[1,])
abline(v = best.mod$means)
abline(v = best.mod$means)
plot(best.mod, type = "both")
a<-mean(best.mod$means[1,])
abline(v = a)
abline(v = a, h=0)
pred<- predict(best.mod, newdata = train.imp)
lda.class<-pred.valid$class
lda.class<-pred$class
lda.class
pos<-train.imp[lda.class==1]
pos<-train.imp[lda.class=1]
pos<-train.imp[lda.class=1,]
pos<-train.imp[lda.class==1,]
neg<-train.imp[lda.class==0,]
plot(neg$sq.m.h,neg$sent)
hist(pos$price)
plot(best.mod, type = "both")
best.mod$means
plot(best.mod, type = "both")
pred<- predict(best.mod, newdata = train.imp)
lda.class<-pred$class
group0<-train.imp[lda.class==1,]
group1<-train.imp[lda.class==0,]
hist(group1,probability = True)
hist(group1$sq.m.h,probability = True)
hist(group1$sq.m.h,probability = TRUE)
lines(group$sq.m.h)
lines(group1$sq.m.h)
hgA <- hist(group0, breaks = ax, plot = FALSE) # Save first histogram data
group0<-train.imp[lda.class==1,]$sq.m.h
group1<-train.imp[lda.class==0,]$sq.m.h
hgA <- hist(group0, breaks = ax, plot = FALSE) # Save first histogram data
hgA <- hist(group0, plot = FALSE) # Save first histogram data
hgB <- hist(group1, plot = FALSE) # Save 2nd histogram data
plot(hgA, col = c1) # Plot 1st histogram using a transparent color
plot(hgB, col = c2, add = TRUE) # Add 2nd histogram using different color
plot(hgA) # Plot 1st histogram using a transparent color
plot(hgB, add = TRUE) # Add 2nd histogram using different color
plot(hgA,col=rgb(0,0,1,1/4)) # Plot 1st histogram using a transparent color
plot(hgB, col=rgb(1,0,0,1/4),add = TRUE) # Add 2nd histogram using different color
abline(v = mean(best.mod$means[,1])
abline(v = mean(best.mod$means[,1]))
abline(v = mean(best.mod$means[,1]))
abline(v = mean(best.mod$means[,1]),col = "orange")
train.imp <-
read.csv(file="train.imp.csv",header=TRUE,stringsAsFactors=T)
test.imp <-
read.csv(file="test.imp.csv",header=TRUE,stringsAsFactors=T)
train.scale<-as.data.frame(scale(train.imp))
cv.out<-method.fwd("lda",train.scale,10)
ridge.mod
lasso.mod
lasso.mod$
calls
lasso.mod$a0
lasso.mod$beta
cv.out$lambda.min
cv.out$lambda
which(cv.out$lambda)
which.min(cv.out$lambda)
lasso.mod$beta[67]
lasso.mod$beta[67,]
lasso.mod$beta
lasso.mod$beta[,67]
barchart(l[1,-4],colnames(l[1,-4]),col = "green")
l<-cbind(0.4341238,0.443308, 0.4342615, 19051.64, 0.2850119, 0.2764581,0.3419989,0.2474197)
colnames(l)<-c("Linear regression","Ridge regression","Lasso Regression",
"Decision Tree","Bagging","Random Forest","Boost","XGboost")
barchart(l[1,-4],colnames(l[1,-4]),col = "green")
q<-cbind(1-0.2038667,1-0.2038667,1-0.2066333)
colnames(q)<-c("logistic regression","LDA","QDA")
barchart(q[1,],colnames(q[1,]),col = "lightblue")
q<-cbind(0.6879958,1-0.283943)
colnames(q)<-c("Boost tree","XGboost")
barchart(q[1,],colnames(q[1,]),col = "orange")
?boost
?class())
?class()
class?
()
class()?
?pbinom()
pbinom(55,60,0.1)
pbinom(55,60,0.1)
pbinom(5,60,0.1)
pbinom(55,60,0.1)
pbinom(55,60,0.9)
pbinom(5,60,0.1)
pbinom(5,60,0.1)
pbinom(55,60,0.9)
pbinom(55,60,0.1)
pbinom(60,60,0.1)
pbinom(49,60,0.1)
pbinom(30,60,0.1)
pbinom(20,60,0.1)
pbinom(1,60,0.1)
pbinom(6,60,0.1)
pbinom(5,60,0.1)
pbinom(55,60,0.9)
pbinom(55,60,0.1,lower.tail = FALSE)
pbinom(5,60,0.1,lower.tail = FALSE)
pbinom(5,60,0.1)
list = list(0,0.025,0.05,0.075,0.1)
list
for(i in list){}
length(list)
x = rep(0,5)
for(i in 1:length(list){
x = rep(0,5)
for(i in 1:length(list)){
x[i] = pbinom(5,60,list[i])
}
list[1]
list[2]
list[3]
x = rep(0,5)
for(i in 2:length(list)){
x[i] = pbinom(5,60,list[i])
}
sum(x)
x
list
list[1]
list[2]
for(i in 2:length(list)){
x[i] = pbinom(5,60,list[i])
}
pbinom(5, 60, list[2])
pbinom(5, 60, list[3])
pbinom(5, 60, list[4])
list[4]
x = rep(0,5)
list<-c(0,0.025,0.05,0.075,0.1)
for(i in 2:length(list)){
x[i] = pbinom(5,60,list[i])
}
sum(x)
pbinom(5,60,list[1])
pbinom(5,60,list[2])
pbinom(5,60,list[3])
pbinom(5,60,list[4])
pbinom(5,60,list[5])
t = pbinom(5,60,list[5])
t*0.015/sum(x)
55+58+59+54+56+57+57+50+52+60
600-558
x = rep(0,5)
list<-c(0,0.025,0.05,0.075,0.1)
for(i in 2:length(list)){
x[i] = pbinom(42,600,list[i])
}
sum(x)
x
glist<-c(0.80,0.1,0.05,0.035,0.015)
for(i in 1:length(list)){
x[i] = pbinom(42,600,list[i])
}
sum(x)
x
t<-rep(0,5)
glist<-c(0.80,0.1,0.05,0.035,0.015)
t<-rep(0,5)
for(i in 1:length(glist)){
t = pbinom(42,600,list[5])*glist[i]/sum(x)
}
t
t<-rep(0,5)
t[i] = pbinom(42,600,list[5])*glist[i]/sum(x)
for(i in 1:length(glist)){
t[i] = pbinom(42,600,list[5])*glist[i]/sum(x)
}
t
max(t)
which.max(t)
glist<-c(0.80,0.1,0.05,0.035,0.015)
for(i in 1:length(list)){
x[i] = pbinom(42,600,list[i])*glist[1]
}
sum(x)
x
pbinom(5,60,list[5])*glist[i]/sum(x)
pbinom(5,60,list[5])
glist[5]
sum(x)
x
for(i in 2:length(list)){
x[i] = pbinom(42,600,list[i])*glist[1]
}
sum(x)
pbinom(5,60,list[5])*glist[5]/sum(x)
for(i in 2:length(list)){
x[i] = pbinom(42,600,list[i])*glist[i]
}
sum(x)
x
x = rep(0,5)
list<-c(0,0.025,0.05,0.075,0.1)
glist<-c(0.80,0.1,0.05,0.035,0.015)
for(i in 2:length(list)){
x[i] = pbinom(42,600,list[i])*glist[i]
}
sum(x)
x
pbinom(5,60,list[5])*glist[5]/sum(x)
t<-rep(0,5)
for(i in 1:length(glist)){
t[i] = pbinom(42,600,list[i])*glist[i]
}
t
for(i in 2:length(glist)){
t[i] = pbinom(42,600,list[i])*glist[i]
}
t
t<-rep(0,5)
for(i in 2:length(glist)){
t[i] = pbinom(42,600,list[i])*glist[i]
}
t
tpost<-rep(0,5)
tpost<-rep(0,5)
for(i in 1:length(glist)){
tpost[i] = pbinom(42,600,list[i])*glist[i]/sum(t)
}
tpost
tpost<-rep(0,5)
for(i in 2:length(glist)){
tpost[i] = pbinom(42,600,list[i])*glist[i]/sum(t)
}
tpost
ep<-sum(list*glist)
ep
et<-sum(t*tpost)
et
et<-sum(list*tpost)
et
library (glmnet)
setwd("E:\ANU Sem 3\STAT7050\A1")
setwd("E:/ANU Sem 3/STAT7050/A1")
setwd("E:/ANU Sem 3/STAT7050/A1")
data <-
read.csv(file="prostate.data.txt",header=TRUE)
View(data)
data <-
read.table(file="prostate.data.txt",header=TRUE)
View(data)
str(data)
train<-data$train==True
train<-data[data$train==True]
setwd("E:/ANU Sem 3/STAT7050/A1")
data <-
read.table(file="prostate.data.txt",header=TRUE)
str(data)
train<-data[data$train==Tr]
View(data)
train<-data[data$train==TRUE]
data$train==TRUE
train<-data[data$train==TRUE,]
test<-data[data$train==FLASE,]
test<-data[data$train==FALSE,]
nrow(train)
nrow(test)
nrow(data)
View(train)
train<-data[data$train==TRUE,-train]
train<-data[data$train==TRUE,-data$train]
train<-data[data$train==TRUE,]
train<-data[data$train==TRUE,][,-"train"]
data[data$train==TRUE,]
data[data$train==TRUE,][,-"train"]
train<-data[data$train==TRUE,][-"train"]
train<-data[data$train==TRUE,][-train]
train<-data[data$train==TRUE,]
test<-data[data$train==FALSE,]
train<-train[-train]
train<-train[-"train"]
train<-train[,1:9]
test<-test[,1:9]
y<-train$lpsa
OLS<-lm(y~.,data = train)
summary(OLS)
x <- model.matrix (lpsa ~., data = train)
x <- model.matrix (lpsa ~., data = train)
View(x)
y<-train$lpsa
OLS<-lm(y~.,data = train)
summary(OLS)
pairs(train)
x <- model.matrix (lpsa ~., data = train)
ridge <- glmnet (x, y, alpha = 0)
set.seed (1)
cv.out <- cv.glmnet (x, y, alpha = 0)
plot (cv.out)
bestlam <- cv.out$lambda.min
bestlam
x.test <- model.matrix (lpsa~.,test)
ridge.pred <- predict (ridge , s = bestlam ,
newx = x.test)
ridge.pred
?mse
?mse()
mse.ridge<-mean((test$lpsa-ridge.pred)^2)
mse.ridge
lasso <- glmnet (x, y, alpha = 1)
plot (lasso)
set.seed (1)
cv.out <- cv.glmnet (x, y, alpha = 1)
plot (cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.pred <- predict (lasso , s = bestlam ,
newx = x.test)
mse.lasso<-mean((test$lpsa-lasso.pred)^2)
mse.ridge
mse.lasso
OLS.pred<-predict(OLS,newdata = test)
mse.OLS<-mean((test$lpsa-OLS.pred)^2)
mse.OLS
OLS.pred<-predict(OLS,newdata = test)
mse.OLS<-mean((test$lpsa-OLS.pred)^2)
mse.OLS
OLS.pred
length(OLS)
length(OLS.pred)
nrow(test)
test
y<-train$lpsa
OLS<-lm(y~.,data = train)
summary(OLS)
OLS.pred<-predict(OLS,newdata = test)
mse.OLS<-mean((test$lpsa-OLS.pred)^2)
mse.OLS
test$lpsa-lasso.pred)^2
(test$lpsa-lasso.pred)^2
str(train)
hist(train$lcavol)
?glmnet
elasticnet <- glmnet (x, y)
elasticnet
plot (elasticnet)
elasticnet <- glmnet (x, y)
summary(elasticnet)
cv.out <- cv.glmnet (x, y)
plot (cv.out)
bestlam <- cv.out$lambda.min
bestlam
bestlam <- cv.out$lambda.min
bestlam
elasticnet.pred <- predict (elasticnet , s = bestlam ,
newx = x.test)
mse.elasticnet<-mean((test$lpsa-elasticnet.pred)^2)
mse.elasticnet
cv.out <- cv.glmnet (x, y,type.measure = "mse")
plot (cv.out)
cv.glmnet()
?cv.glmnet
cv.out <- cv.glmnet (x, y, alpha = 1,,type.measure = "mse")
plot (cv.out)
bestlam <- cv.out$lambda.min
bestlam
set.seed (1)
cv.out <- cv.glmnet (x, y, alpha = 0)
plot (cv.out)
bestlam <- cv.out$lambda.min
bestlam
cv.out <- cv.glmnet (x, y, alpha = 0,,type.measure = "mse")
plot (cv.out)
bestlam <- cv.out$lambda.min
bestlam
set.seed (1)
cv.out <- cv.glmnet (x, y, alpha = 0,,type.measure = "mse")
plot (cv.out)
bestlam <- cv.out$lambda.min
bestlam
elasticnet$lambda
elasticnet$call
dim(train)
coef(ridge, s = "lambda.min")
coef(ridge, s = bestlam)
coef(lasso, s = bestlam)
x
coef(elasticnet, s = bestlam)
coef(lasso, s = bestlam)
