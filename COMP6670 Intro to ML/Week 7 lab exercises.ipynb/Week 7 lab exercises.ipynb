{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMP3670/6670 Tutorial Week 7 - Regression**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THEORY SECTION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression and gradient descent are pillars in machine learning.  \n",
    "\n",
    "The first part of this tutorial to go over the lecture slides in regression and gradient descent.  \n",
    "\n",
    "**PRIORITIES:**\n",
    "1. Ensure you understand **every step** of (stochastic) gradient desent.\n",
    "2. Ensure you could can derive the gradient of regression problems with or without regression. \n",
    "\n",
    "Once that's all done, revisit the clustering section. Did you understand everything here as well? Unsupervised learning is important because it not only has immediate practical uses but is more relevant to the development of AGI (Artificial General Intelligence) than supervised (labelled) learning. If you don't understand this paragraph, please ask your tutor to clarify the meaning of supervised and unsupservised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROGRAMMING SECTION**\n",
    "---\n",
    "\n",
    "We're going to do a simple scalar linear regression with gradient descent.\n",
    "\n",
    "\n",
    "-----------\n",
    "\n",
    "   **TASK:** \n",
    "   \n",
    "   1. Randomly generate a matrix $X \\in \\mathbb{R}^{m \\times n}$, where each row of $X$ is a training example.\n",
    "   2. Choose a vector $t \\in \\mathbb{R}^{n \\times 1}$.\n",
    "   3. Generate $Y$ by $Xt = Y$.\n",
    "   4. Then generate a random matrix $\\theta \\in \\mathbb{R}^{n \\times 1}$.\n",
    "   5. Implement gradient descent to approximate $t$ with $\\theta$.\n",
    "   6. Check your gradient descent algorithm correctly approximated $t$. Talk to your classmates and tutor to make sure if you're unsure.\n",
    "   7. Verify your answer with the closed form solution employing the Moore-Penrose inverse.\n",
    "   \n",
    "Note that in the above we're essentially pretending we don't know $t$. Obviously, if we have $t$, linear regression with gradient descent would be unnecessary, but the point is to help you understand what gradient descent is doing.\n",
    "\n",
    "Also note: we should use the squared loss function, computed as the square of the difference between the predicted function values and the observed function values (or ground truth). \n",
    "\n",
    "\n",
    "-----------\n",
    "\n",
    "**GENERAL COURSE HINTS:** \n",
    "- $n$ can be any number you like, but be reasonable.\n",
    "- If you need extra study materials, Stanford and MIT both have some amazing freely available course content online. Look up \"Machine Learning Stanford CS229\" or \"CS221\" or \"CS221N\" for details.\n",
    "- Wikipedia is your friend. It's not always right, but it's always there for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\yangs\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yangs\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\yangs\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yangs\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\yangs\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\yangs\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\yangs\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\yangs\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "! pip install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86417955]\n",
      " [0.89924213]\n",
      " [0.79641104]\n",
      " [0.61700092]\n",
      " [0.63738377]]\n",
      "[[0.09796915]\n",
      " [0.98491521]\n",
      " [0.91701654]\n",
      " [0.05733517]\n",
      " [0.26664093]]\n",
      "[[0.09777152]\n",
      " [0.98472275]\n",
      " [0.91735593]\n",
      " [0.05724705]\n",
      " [0.26665602]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE.\n",
    "m = 5\n",
    "n = 5\n",
    "X = np.random.rand(m,n)\n",
    "t = np.random.rand(n,1)\n",
    "Y = X@t\n",
    "theta = np.random.rand(n,1)\n",
    "print(theta)\n",
    "def grad(X, theta, Y):\n",
    "    # YOUR CODE HERE\n",
    "    return np.matrix.transpose(np.matrix.transpose(X@theta-Y)@(X)/len(Y))\n",
    "def grad_descent(X, Y, theta,iterations, learning_rate):\n",
    "    i=0\n",
    "    theta = theta\n",
    "    while(i<iterations):\n",
    "        theta = theta - learning_rate*grad(X, theta, Y)\n",
    "        i+=1\n",
    "    return theta\n",
    "new_theta = grad_descent(X, Y, theta,5000, 0.2)\n",
    "print(new_theta)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "**Extended Task:** We study various influence factors in training a linear regression model in this task.\n",
    "\n",
    "1. noise. When collecting real-world data, it is common that there would be noise included. Adding noise to your generated data and see how would this influence the parameter estimation.\n",
    "\n",
    "2. sample amount. Sometimes it is expensive to collect data while a lot of parameters need to be trained. Let's study the effect of training example amount. That is changing the $m$ for $X \\in \\mathbb{R}^{m \\times n}$ and compare the final loss fixing training epoch and learning rate. \n",
    "\n",
    "3. learning rate. How would the learning rate influence the convergence of the optimization process?\n",
    "\n",
    "-----------\n",
    "**Hint**\n",
    "- You can add noise by settting $Y=Xt+\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(\\mu,\\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial Y is  [[1.87689516]\n",
      " [0.78796779]\n",
      " [2.09011557]\n",
      " [1.56566321]\n",
      " [1.46735578]]\n",
      "The initial theta is  [[0.02358686]\n",
      " [0.21625371]\n",
      " [0.24875499]\n",
      " [0.10278459]\n",
      " [0.12265653]]\n",
      "After we add a normal error to the Y, the outcome is  [[0.02538885]\n",
      " [0.93488344]\n",
      " [1.25034703]\n",
      " [0.16142739]\n",
      " [0.26647301]]\n",
      "The initial t is  [[0.004002  ]\n",
      " [0.98574362]\n",
      " [0.75081542]\n",
      " [0.36654095]\n",
      " [0.60710437]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE.\n",
    "\n",
    "# Adding noise to your generated data \n",
    "m = 5\n",
    "n = 5\n",
    "X = np.random.rand(m,n)\n",
    "t = np.random.rand(n,1)\n",
    "mu,sigma = 0,0.1\n",
    "e = np.random.normal(mu, sigma, size=(m, 1))\n",
    "Y = X@t+e\n",
    "print(\"The initial Y is \",Y)\n",
    "theta = np.random.rand(n,1)\n",
    "print(\"The initial theta is \",theta)\n",
    "\n",
    "new_theta = grad_descent(X, Y, theta,20000, 0.1)\n",
    "print(\"After we add a normal error to the Y, the outcome is \",new_theta)# Add the small error into the Y degrade all the theta outcome\n",
    "print(\"The initial t is \",t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial Y is  [[0.91332575]\n",
      " [1.0815174 ]\n",
      " [1.67551892]\n",
      " [0.59464297]\n",
      " [1.52280865]\n",
      " [0.61340883]\n",
      " [1.56566667]\n",
      " [1.0495464 ]\n",
      " [0.72370564]\n",
      " [1.19331662]\n",
      " [1.22240439]\n",
      " [1.02689012]\n",
      " [0.68425347]\n",
      " [1.1020185 ]\n",
      " [1.43416533]\n",
      " [0.94068382]\n",
      " [1.32119537]\n",
      " [0.46817941]\n",
      " [1.14845274]\n",
      " [1.01399539]\n",
      " [1.20230635]\n",
      " [1.01689513]\n",
      " [1.05620694]\n",
      " [0.72274382]\n",
      " [1.25726859]\n",
      " [1.51775101]\n",
      " [0.79738437]\n",
      " [0.57433226]\n",
      " [1.30551421]\n",
      " [1.31442523]\n",
      " [0.38517785]\n",
      " [1.38005087]\n",
      " [1.06508577]\n",
      " [1.32238513]\n",
      " [1.35633212]\n",
      " [0.8203235 ]\n",
      " [1.43910035]\n",
      " [1.16243306]\n",
      " [0.84352568]\n",
      " [0.80192598]\n",
      " [1.20779304]\n",
      " [1.09080915]\n",
      " [0.83154598]\n",
      " [1.33653659]\n",
      " [1.6080151 ]\n",
      " [1.16507139]\n",
      " [0.92617373]\n",
      " [0.90977508]\n",
      " [1.15088573]\n",
      " [1.49312828]\n",
      " [1.04292199]\n",
      " [1.10313388]\n",
      " [1.27225881]\n",
      " [1.26653351]\n",
      " [1.67632866]\n",
      " [1.41288644]\n",
      " [1.04312378]\n",
      " [0.84749094]\n",
      " [1.08547789]\n",
      " [0.73029364]\n",
      " [1.12525954]\n",
      " [1.00279579]\n",
      " [1.2385895 ]\n",
      " [1.13119156]\n",
      " [1.53685053]\n",
      " [1.21689394]\n",
      " [1.32543221]\n",
      " [0.69015676]\n",
      " [0.88001325]\n",
      " [0.48912674]\n",
      " [1.50283265]\n",
      " [1.24738372]\n",
      " [1.6058442 ]\n",
      " [0.98402667]\n",
      " [1.33454469]\n",
      " [0.61854646]\n",
      " [1.14602335]\n",
      " [1.47523495]\n",
      " [0.39694968]\n",
      " [1.21593777]\n",
      " [0.26187645]\n",
      " [0.7078459 ]\n",
      " [1.13678377]\n",
      " [1.80055716]\n",
      " [1.01831583]\n",
      " [0.96662287]\n",
      " [0.8181851 ]\n",
      " [0.57661614]\n",
      " [1.23443673]\n",
      " [1.13948314]\n",
      " [0.90733993]\n",
      " [1.42419817]\n",
      " [1.181699  ]\n",
      " [1.26074624]\n",
      " [1.31249323]\n",
      " [0.84558574]\n",
      " [1.03248359]\n",
      " [1.65769043]\n",
      " [1.48906355]\n",
      " [1.18631699]]\n",
      "The initial theta is  [[0.02026886]\n",
      " [0.46986541]\n",
      " [0.10215775]\n",
      " [0.84913754]\n",
      " [0.31851987]]\n",
      "After changing sample amount, the outcome theta is  [[0.2396413 ]\n",
      " [0.41379861]\n",
      " [0.61041912]\n",
      " [0.27474291]\n",
      " [0.6029541 ]]\n",
      "The initial t is  [[0.29564629]\n",
      " [0.4394597 ]\n",
      " [0.62138013]\n",
      " [0.25011015]\n",
      " [0.55253951]]\n"
     ]
    }
   ],
   "source": [
    "## change sample amount\n",
    "m = 100\n",
    "n = 5\n",
    "X = np.random.rand(m,n)\n",
    "t = np.random.rand(n,1)\n",
    "mu,sigma = 0,0.1\n",
    "e = np.random.normal(mu, sigma, size=(m, 1))\n",
    "Y = X@t +e\n",
    "print(\"The initial Y is \",Y)\n",
    "theta = np.random.rand(n,1)\n",
    "print(\"The initial theta is \",theta)\n",
    "\n",
    "new_theta = grad_descent(X, Y, theta,10000, 0.1)\n",
    "print(\"After changing sample amount, the outcome theta is \",new_theta)# Add the small error into the Y degrade all the theta outcome\n",
    "print(\"The initial t is \",t)\n",
    "# Add sample size imporve our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial t is  [[0.59077559]\n",
      " [0.42660289]\n",
      " [0.69155614]\n",
      " [0.86850741]\n",
      " [0.77219976]]\n",
      "With 5 iteration, the outcome theta is  [[0.57198403]\n",
      " [0.46781438]\n",
      " [0.75894822]\n",
      " [0.8219425 ]\n",
      " [0.74691118]]\n",
      "With 10 iteration, the outcome theta is  [[0.5718215 ]\n",
      " [0.4677852 ]\n",
      " [0.75923962]\n",
      " [0.82167408]\n",
      " [0.74702282]]\n",
      "With 20 iteration, the outcome theta is  [[0.57182116]\n",
      " [0.46778529]\n",
      " [0.75924009]\n",
      " [0.8216736 ]\n",
      " [0.74702295]]\n"
     ]
    }
   ],
   "source": [
    "m = 100\n",
    "n = 5\n",
    "X = np.random.rand(m,n)\n",
    "t = np.random.rand(n,1)\n",
    "mu,sigma = 0,0.1\n",
    "e = np.random.normal(mu, sigma, size=(m, 1))\n",
    "Y = X@t +e\n",
    "print(\"The initial t is \",t)\n",
    "\n",
    "\n",
    "new_theta = grad_descent(X, Y, theta,500, 0.2)\n",
    "print(\"With 5 iteration, the outcome theta is \",new_theta)# Add the small error into the Y degrade all the theta outcome\n",
    "\n",
    "new_theta = grad_descent(X, Y, theta,1000, 0.2)\n",
    "print(\"With 10 iteration, the outcome theta is \",new_theta)# Add the small error into the Y degrade all the theta outcome\n",
    "\n",
    "new_theta = grad_descent(X, Y, theta,20000, 0.2)\n",
    "print(\"With 20 iteration, the outcome theta is \",new_theta)# Add the small error into the Y degrade all the theta outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
