% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Assignment 1 for STAT7050},
  pdfauthor={Songze Yang, u7192786},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Assignment 1 for STAT7050}
\author{Songze Yang, u7192786}
\date{2022-08-04}

\begin{document}
\maketitle

The prostate cancer data set contains 9 variables. We are interested in
the lpsa, which is the logarithm of prostate-specific antigen. We split
the data into the training and testing data by the last colmun.

\hypertarget{question-1}{%
\subsection{Question 1}\label{question-1}}

Firstly, we apply OLS, ridge regression, the lasso, the naive elastic
net and the elastic net to the data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OLS}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(lpsa}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data =}\NormalTok{ train)}
\NormalTok{OLS.pred}\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(OLS,}\AttributeTok{newdata =}\NormalTok{ test)}
\NormalTok{mse.OLS}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{((test}\SpecialCharTok{$}\NormalTok{lpsa}\SpecialCharTok{{-}}\NormalTok{OLS.pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{coef.OLS}\OtherTok{\textless{}{-}}\FunctionTok{coef}\NormalTok{(OLS)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{ (x, y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{ (}\DecValTok{1}\NormalTok{)}
\NormalTok{cv.out }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{ (x, y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{,}\AttributeTok{type.measure =} \StringTok{"mse"}\NormalTok{)}
\NormalTok{bestlam.ridge }\OtherTok{\textless{}{-}}\NormalTok{ cv.out}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{ridge.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{ (ridge , }\AttributeTok{s =}\NormalTok{ bestlam.ridge ,}
                       \AttributeTok{newx =}\NormalTok{ x.test)}
\NormalTok{mse.ridge}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{((test}\SpecialCharTok{$}\NormalTok{lpsa}\SpecialCharTok{{-}}\NormalTok{ridge.pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{coef.ridge}\OtherTok{\textless{}{-}}\FunctionTok{coef}\NormalTok{(ridge, }\AttributeTok{s =}\NormalTok{ bestlam.ridge)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{ (x, y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv.out }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{ (x, y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{,}\AttributeTok{type.measure =} \StringTok{"mse"}\NormalTok{)}
\NormalTok{bestlam.lasso }\OtherTok{\textless{}{-}}\NormalTok{ cv.out}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{lasso.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{ (lasso , }\AttributeTok{s =}\NormalTok{ bestlam.lasso,}
                       \AttributeTok{newx =}\NormalTok{ x.test)}
\NormalTok{mse.lasso}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{((test}\SpecialCharTok{$}\NormalTok{lpsa}\SpecialCharTok{{-}}\NormalTok{lasso.pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{coef.lasso}\OtherTok{\textless{}{-}}\FunctionTok{coef}\NormalTok{(lasso, }\AttributeTok{s =}\NormalTok{ bestlam.lasso)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid.search}\OtherTok{\textless{}{-}}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\AttributeTok{by =} \FloatTok{0.001}\NormalTok{) }\CommentTok{\#grid search algorithm}
\NormalTok{cv.result}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FunctionTok{length}\NormalTok{(grid.search)}\SpecialCharTok{*}\DecValTok{2}\NormalTok{),}\FunctionTok{length}\NormalTok{(grid.search),}\DecValTok{3}\NormalTok{)}\DocumentationTok{\#\#matrix to store cv, lambda and alpha value.}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(grid.search))\{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  cv.out }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{ (x, y,}\AttributeTok{alpha =}\NormalTok{ grid.search[i])}
\NormalTok{  cv.result[i,}\DecValTok{1}\NormalTok{]}\OtherTok{\textless{}{-}}\NormalTok{cv.out}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{  cv.result[i,}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{min}\NormalTok{(cv.out}\SpecialCharTok{$}\NormalTok{cvm)}
\NormalTok{  cv.result[i,}\DecValTok{3}\NormalTok{]}\OtherTok{\textless{}{-}}\NormalTok{grid.search[i]}
\NormalTok{\}}

\NormalTok{best.lam.elanet}\OtherTok{\textless{}{-}}\NormalTok{cv.result[,}\DecValTok{1}\NormalTok{][}\FunctionTok{which.min}\NormalTok{(cv.result[,}\DecValTok{2}\NormalTok{])]}
\NormalTok{best.alpha.elanet}\OtherTok{\textless{}{-}}\NormalTok{cv.result[,}\DecValTok{3}\NormalTok{][}\FunctionTok{which.min}\NormalTok{(cv.result[,}\DecValTok{2}\NormalTok{])]}

\NormalTok{elasticnet }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{ (x, y, }\AttributeTok{alpha =}\NormalTok{ best.alpha.elanet)}\DocumentationTok{\#\# navie elastic net}
\NormalTok{elasticnet.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{ (elasticnet , }\AttributeTok{s =}\NormalTok{ best.lam.elanet ,}\AttributeTok{newx =}\NormalTok{ x.test)}
\NormalTok{mse.elasticnet}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{((test}\SpecialCharTok{$}\NormalTok{lpsa}\SpecialCharTok{{-}}\NormalTok{elasticnet.pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{coef.naive}\OtherTok{\textless{}{-}}\FunctionTok{coef}\NormalTok{(elasticnet, }\AttributeTok{s =}\NormalTok{ best.lam.elanet)}

\NormalTok{lambda2}\OtherTok{\textless{}{-}}\NormalTok{best.lam.elanet}\SpecialCharTok{*}\NormalTok{((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{best.alpha.elanet))}\DocumentationTok{\#\#elastic net}
\NormalTok{coef.elanet}\OtherTok{\textless{}{-}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{lambda2)}\SpecialCharTok{*}\NormalTok{coef.naive}

\NormalTok{x.t.mat}\OtherTok{\textless{}{-}}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{,x.test))}
\NormalTok{els.pred}\OtherTok{\textless{}{-}}\NormalTok{x.t.mat}\SpecialCharTok{\%*\%}\NormalTok{coef.elanet}
\NormalTok{mse.elsnet}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{((test}\SpecialCharTok{$}\NormalTok{lpsa}\SpecialCharTok{{-}}\NormalTok{els.pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The objective function in the glm package is shown as below.

\[
arg min \frac{1}{2}|y − Xβ|^2+\lambda ((1-α)/2||β||_2^2+α||β||_1)
\]

So the coefficient of naive elastic net is calculated as following.

\[
arg min |y − Xβ|^2+\lambda_2 ||β||_2^2+\lambda_1||β||_1)\\ where \\ \lambda_2 = \lambda ((1-α) \\ and\\  \lambda_1 = 2α
\]

The coefficient estimate of elastic net is calculated as:

\[
\theta_(elastic) = (1+\lambda_2)*\theta_(naive)
\]

The result is summarized as below.

\begin{verbatim}
## Loading required package: rhandsontable
\end{verbatim}

\begin{verbatim}
## PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.
\end{verbatim}

\includegraphics{RmarkdwnA1_files/figure-latex/unnamed-chunk-1-1.pdf}

\hypertarget{question-2-10-marks-based-on-the-table-results-completed-in-question-1-which-method-is-the-winner-among-all-the-competitors-in-terms-of-both-prediction-accuracy-and-sparsity-which-method-is-the-worse-method-please-interpret-your-conclusion.}{%
\subsection{Question 2 {[}10 marks{]}: Based on the table results
completed in Question 1, which method is the winner among all the
competitors in terms of both prediction accuracy and sparsity? Which
method is the worse method? Please interpret your
conclusion.}\label{question-2-10-marks-based-on-the-table-results-completed-in-question-1-which-method-is-the-winner-among-all-the-competitors-in-terms-of-both-prediction-accuracy-and-sparsity-which-method-is-the-worse-method-please-interpret-your-conclusion.}}

All of the models except the lasso include all the variables in the
model. The lasso is the best model in terms of the sparsity. The lasso
excludes the gleason, which has a high correlation to our pgg45 and low
correlation to our response. The groups effect collects high correlated
gleason and ppg45 variables and performs a good sparsity for lasso.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{corrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(train))}
\end{Highlighting}
\end{Shaded}

\includegraphics{RmarkdwnA1_files/figure-latex/unnamed-chunk-2-1.pdf}

In terms of the prediction accuracy, the best model is the ridge
regression. It realizes a Test MSE of 0.4943, which is 3 percent above
classical OLS. While the classical OLS is the worst predictive method,
it gives us the unbiased efficient coefficients.

The ridge regression achieves lower Test MSE by performing a
bias-variance trade-off, as followed.

\[
Reducible\ error\ = x_i^T*Var(\theta)*x_i+x_i^T*Bias(\theta)*Bias(\theta)^T*x_i
\]

The OLS method equals bias to 0 but remain the variance. So the ridge
performs a selection between bias and variance in the model to achieve a
lower MSE.

The naive elastic net combines the l-1 and l-2 penalty. It can be
interpret as firstly a ridge type coefficient shrinkage and then a lasso
type variable shrinkage. The ridge type shrinkage will only perform a
bias and variance trade-off in the reducible error of the model. No
variable will shrink to zero after. The lasso will also shrink the
coefficient, however, the coefficients are allowed to minimize to zero
under a l-1 penalty.

The two stage shrinkage for naive elastic net leads to the problem of
over shrinkage. Therefore, a rescaled version of coefficient is
formulated known as elastic net. The elastic net rescales the
coefficient of its naive version by a factor of (1+\lambda2).

However, in our case, every variable is very important in explaining our
response. A lasso type shrinkage in elastic net deteriorates the
predictive power. The naive elastic net over shrink our coefficient
estimate, while the rescaled version remove this problem but also l-1
penalty in elastic net decay its prediction accuracy just like lasso.

Consequently, ridge achieves the best performance.

\hypertarget{question-3}{%
\subsection{Question 3}\label{question-3}}

Both naive elastic net and ridge in our case did not perform any
variable selection. However, the coefficient for naive elastic net is
slightly smaller than those of ridge. If we interpret the naive elastic
net as method that firstly conducts the ridge type variable shrinkage
and then apply a lasso type variable selection. Then the naive elastic
net over shrinks the coefficient.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coef.naive;coef.ridge}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 10 x 1 sparse Matrix of class "dgCMatrix"
##                       s1
## (Intercept)  0.179484975
## (Intercept)  .          
## lcavol       0.516497686
## lweight      0.606195008
## age         -0.016056303
## lbph         0.140243662
## svi          0.695924135
## lcp         -0.140665778
## gleason      0.005693304
## pgg45        0.007675482
\end{verbatim}

\begin{verbatim}
## 10 x 1 sparse Matrix of class "dgCMatrix"
##                       s1
## (Intercept)  0.095549713
## (Intercept)  .          
## lcavol       0.492656404
## lweight      0.601227708
## age         -0.014818243
## lbph         0.137965816
## svi          0.679288020
## lcp         -0.116652810
## gleason      0.017256035
## pgg45        0.007077847
\end{verbatim}

The naive elastic net is like a stretchable fishing net that remain
``all the big fish'' (Zou and Hastie, 2005). Under a combination of l-1
and l-2 penalty, the naive elastic net keeps the gleason variable, which
may be more important to include to our model. This is confirmed by the
Training MSE by ridge(0.4473) and the naive elastic net(0.4434). The
ridge model did a better job in extrapolation but naive elastic net
achieve lower prediction accuracy in the training data.

However, in the data set that is large in number of covariates, the
naive elastic net may perform better as it can present a variable
selection to our data set. The ridge method may fail to select as under
a l-2 penalty, no covariate can be shrinked to zero.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.mse.ridge;train.mse.elasticnet}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4473332
\end{verbatim}

\begin{verbatim}
## [1] 0.4434198
\end{verbatim}

\hypertarget{question-4}{%
\subsection{Question 4}\label{question-4}}

While the classical OLS method gives us the unbiased efficient
coefficient, the ridge regression achieves lower Test MSE by performing
a bias-variance trade-off, as followed.

\[
Reducible\ error\ = x_i^T*Var(\theta)*x_i+x_i^T*Bias(\theta)*Bias(\theta)^T*x_i
\]

The OLS method equals bias to 0 but remain the variance. So the ridge
performs a selection between bias and variance in the model to achieve a
lower MSE. By liberating a certain of bais, a larger amount of variance
can be decreased. Thus, a overall Test MSE will go down and the ridge
overtakes the classical OLS.

\hypertarget{question-5}{%
\subsection{Question 5}\label{question-5}}

The lasso model excludes the gleason. This may come from a grouping
effect for gleason and pgg45. The correlation between the gleason and
pgg45 is as high as 0.7570. By grouping effect, the lasso will selection
only one variable from the two. The lasso losses information regarding
the group of gleason and ppg45, where only one amount is picked.

However, the elastic net perform a trade off between excluding and
including the variable under a l-1 after l-2 shrinkage. In our case, it
is much finer to keep gleason variable than to exclude it. Therefore,
the navie elastic net performs better in our data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{corrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(train))}
\end{Highlighting}
\end{Shaded}

\includegraphics{RmarkdwnA1_files/figure-latex/unnamed-chunk-6-1.pdf}

Further, under l-1 penalty, the estimate in lasso will bias towards to
zero. It is better to refit the model using the selected variable to fit
a lasso model, known as the relaxed lasso. This problem is mild in
elastic net as the estimate is re-scaled by a factor.

\hypertarget{reference}{%
\subsubsection{Reference}\label{reference}}

Zou, H. and Hastie, T., 2005. Regularization and variable selection via
the elastic net. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 67(2), pp.301-320.

\end{document}
